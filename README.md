# MOSMOS: Multi-Organ Segmentation Emerges from Medical Report Supervision

- This repository provides the codebase of work MOSMOS: Multi-Organ Segmentation Emerges from Medical Report Supervision.

- Thanks to a large amount of multi-modal data in modern medical systems, such as medical images and reports, Medical Vision-Language Pre-training (Med-VLP) has demonstrated incredible achievements in coarse-grained downstream tasks (i.e., medical classification, retrieval, and visual question answering). However, the problem of transferring knowledge learned from Med-VLP to fine-grained multi-organ segmentation tasks has barely been investigated. Multi-organ segmentation is challenging mainly due to the lack of large-scale fully annotated datasets and the wide variation in the shape and size of the same organ between individuals with different diseases. In this paper, we propose a novel pre-training & fine-tuning framework for Multi-Organ Segmentation by harnessing the Medical repOrt Supervision (MOSMOS). Specifically, we first introduce global contrastive learning to maximally align the medical image-report pairs in the pre-training stage. To remedy the granularity discrepancy, we further advocate multi-label recognition to implicitly learn the semantic correspondence between image pixels and organ tags. More importantly, our pre-trained models can be transferred to any segmentation model by introducing the pixel-tag attention maps. 2D U-Net and 3D UNETR are utilized to validate the generalization. We have extensively evaluated our approach on BTCV and MMWHS datasets with different modalities. Experimental results in various settings demonstrate the effectiveness of our framework. 

- Code for our proposed approach will be release after the paper is accepted.
